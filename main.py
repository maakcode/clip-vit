import os
from typing import Tuple
import streamlit as st
import faiss
from PIL import Image, ImageFile
from mlx_clip import CLIPModel, CLIPTokenizer, CLIPImageProcessor, convert_clip
from mlx_lm.generate import generate as mlx_generate
from mlx_lm.convert import convert as mlx_convert
from mlx_lm.utils import load as mlx_load

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"


class ClipImageSearch:
    def __init__(self):
        model_name = "openai/clip-vit-large-patch14"
        model_path = f"./models/{model_name}"

        self.index = None
        self.image_names = []
        self.image_objects = []

        st.write(f"ğŸš€ CLIP ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self.model, self.tokenizer, self.img_processor = self._load(
            model_name, model_path
        )
        st.success("âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def _load(
        self, model_name: str, model_path: str
    ) -> Tuple[CLIPModel, CLIPTokenizer, CLIPImageProcessor]:
        if not os.path.isdir(model_path):
            convert_clip(model_name, model_path)

        model = CLIPModel.from_pretrained(model_path)
        tokenizer = CLIPTokenizer.from_pretrained(model_path)
        img_processor = CLIPImageProcessor.from_pretrained(model_path)
        return model, tokenizer, img_processor

    def _get_image_embeddings(self, images: list[ImageFile.ImageFile]):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        rgb_images = [img.convert("RGB") for img in images]
        inputs = {
            "pixel_values": self.img_processor(rgb_images),
        }
        return self.model(**inputs).image_embeds

    def _get_text_embeddings(self, texts: list[str]):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        inputs = {
            "input_ids": self.tokenizer(texts),
        }
        return self.model(**inputs).text_embeds

    def build_index(self, uploaded_files):
        """ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        self.image_names = [f.name for f in uploaded_files]
        self.image_objects = [Image.open(f) for f in uploaded_files]

        embeddings = self._get_image_embeddings(self.image_objects)

        # FAISS index ìƒì„±
        self.index = faiss.IndexFlatIP(embeddings.shape[1])
        self.index.add(embeddings)

        st.success(f"âœ… {len(self.image_names)}ê°œì˜ ì´ë¯¸ì§€ê°€ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def search(self, query: str, top_k: int = 3):
        """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        if self.index is None:
            st.error("âŒ ë¨¼ì € ì´ë¯¸ì§€ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            return []

        text_emb = self._get_text_embeddings([query]).reshape(1, -1)
        D, I = self.index.search(text_emb, top_k)
        results = [
            (self.image_objects[i], self.image_names[i], float(D[0][idx]))
            for idx, i in enumerate(I[0])
        ]
        return results


class LLMTranslator:
    def __init__(self):
        model_name = "ibm-granite/granite-4.0-h-micro"
        model_path = f"./models/{model_name}"

        st.write(f"ğŸš€ ì–¸ì–´ ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self._convert(model_name, model_path)
        self.model, self.tokenizer, *rest = mlx_load(model_path)
        st.success("âœ… ì–¸ì–´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def _convert(self, model_name: str, model_path: str):
        if os.path.isdir(model_path):
            return

        mlx_convert(model_name, quantize=True, mlx_path=model_path)

    def translate(self, text: str) -> str:
        chat = [
            {
                "role": "system",
                "content": "You are a translator. Translate the given text into English only, without any additional comments or responses.",
            },
            {
                "role": "user",
                "content": text,
            },
        ]

        prompt = self.tokenizer.apply_chat_template(chat, add_generation_prompt=True)
        text = mlx_generate(
            self.model,
            self.tokenizer,
            prompt=prompt,
        )
        return text


def main():
    # ============================================
    # Streamlit UI
    # ============================================
    st.set_page_config(page_title="Image Search", layout="centered")
    st.title("ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰")
    st.caption("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    # ìºì‹±ëœ ëª¨ë¸ ì‚¬ìš©
    @st.cache_resource
    def load_app():
        return ClipImageSearch()

    @st.cache_resource
    def load_translator():
        return LLMTranslator()

    app = load_app()
    translator = load_translator()

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_files = st.file_uploader(
        "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["png", "jpg", "jpeg", "webp"],
        accept_multiple_files=True,
    )

    # ì¸ë±ìŠ¤ ìƒì„±
    if uploaded_files:
        st.write("ğŸ“¦ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€:")
        cols = st.columns(5)
        for i, file in enumerate(uploaded_files):
            img = Image.open(file)
            cols[i % 5].image(img, caption=file.name, width="stretch")

        if st.button("ğŸ§  ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ìƒì„±"):
            app.build_index(uploaded_files)

    # ê²€ìƒ‰
    if app.index is not None:
        query = st.text_input(
            "ê²€ìƒ‰í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'íŒŒë€ ëˆˆì„ ê°€ì§„ ê·€ì—¬ìš´ ê³ ì–‘ì´')"
        )
        top_k = st.slider("ê²€ìƒ‰í•  ìƒìœ„ ì´ë¯¸ì§€ ê°œìˆ˜", 1, 10, 3)

        if st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰"):
            translated_query = translator.translate(query)
            st.write(f"{query} {translated_query}")
            results = app.search(translated_query, top_k)
            if results:
                st.write("### ê²€ìƒ‰ ê²°ê³¼:")
                cols = st.columns(top_k)

                for idx, (img, name, score) in enumerate(results):
                    cols[idx].image(
                        img,
                        caption=f"{name}\nìœ ì‚¬ë„: {score:.3f}",
                        use_container_width=True,
                    )


if __name__ == "__main__":
    main()
