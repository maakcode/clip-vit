import streamlit as st
import torch
import numpy as np
import faiss
from PIL import Image, ImageFile
from transformers import (
    CLIPProcessor,
    CLIPModel,
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline,
)
import os

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"
device = "cuda" if torch.cuda.is_available() else "cpu"


class ClipImageSearch:
    def __init__(self):
        model_name = "openai/clip-vit-large-patch14"

        self.index = None
        self.image_names = []
        self.image_objects = []

        st.write(f"ğŸš€ CLIP ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        st.success("âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def _get_image_embeddings(self, images: list[ImageFile.ImageFile]):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        rgb_images = [img.convert("RGB") for img in images]
        inputs = self.processor(
            images=rgb_images, return_tensors="pt", padding=True
        ).to(device)

        features = None
        with torch.inference_mode():
            features = self.model.get_image_features(**inputs)

        features = features / features.norm(dim=-1, keepdim=True)
        return features.cpu().numpy().astype("float32")

    def _get_text_embeddings(self, texts: list[str]):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        inputs = self.processor(text=texts, return_tensors="pt", padding=True).to(
            device
        )

        features = None
        with torch.inference_mode():
            features = self.model.get_text_features(**inputs)

        features = features / features.norm(dim=-1, keepdim=True)
        return features.cpu().numpy().astype("float32")

    def build_index(self, uploaded_files):
        """ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        self.image_names = [f.name for f in uploaded_files]
        self.image_objects = [Image.open(f) for f in uploaded_files]

        embeddings = self._get_image_embeddings(self.image_objects)

        # FAISS index ìƒì„±
        self.index = faiss.IndexFlatIP(embeddings.shape[1])
        self.index.add(embeddings)

        st.success(f"âœ… {len(self.image_names)}ê°œì˜ ì´ë¯¸ì§€ê°€ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def search(self, query: str, top_k: int = 3):
        """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        if self.index is None:
            st.error("âŒ ë¨¼ì € ì´ë¯¸ì§€ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            return []

        text_emb = self._get_text_embeddings([query]).reshape(1, -1)
        D, I = self.index.search(text_emb, top_k)
        results = [
            (self.image_objects[i], self.image_names[i], float(D[0][idx]))
            for idx, i in enumerate(I[0])
        ]
        return results


class LLMTranslator:
    def __init__(self):
        model_name = "ibm-granite/granite-4.0-h-micro"

        st.write(f"ğŸš€ ì–¸ì–´ ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)
        self.pipeline = pipeline(
            "text-generation", model=self.model, tokenizer=self.tokenizer
        )
        st.success("âœ… ì–¸ì–´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def translate(self, text: str) -> str:
        chat = [
            {
                "role": "system",
                "content": "You are a translator. Translate the given text into English only, without any additional comments or responses.",
            },
            {
                "role": "user",
                "content": text,
            },
        ]

        outputs = self.pipeline(chat, max_new_tokens=100)
        return outputs[0]["generated_text"][-1]["content"]


def main():
    # ============================================
    # Streamlit UI
    # ============================================
    st.set_page_config(page_title="Image Search", layout="centered")
    st.title("ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰")
    st.caption("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    # ìºì‹±ëœ ëª¨ë¸ ì‚¬ìš©
    @st.cache_resource
    def load_app():
        return ClipImageSearch()

    @st.cache_resource
    def load_translator():
        return LLMTranslator()

    app = load_app()
    translator = load_translator()

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_files = st.file_uploader(
        "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["png", "jpg", "jpeg", "webp"],
        accept_multiple_files=True,
    )

    # ì¸ë±ìŠ¤ ìƒì„±
    if uploaded_files:
        st.write("ğŸ“¦ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€:")
        cols = st.columns(5)
        for i, file in enumerate(uploaded_files):
            img = Image.open(file)
            cols[i % 5].image(img, caption=file.name, width="stretch")

        if st.button("ğŸ§  ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ìƒì„±"):
            app.build_index(uploaded_files)

    # ê²€ìƒ‰
    if app.index is not None:
        query = st.text_input(
            "ê²€ìƒ‰í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'íŒŒë€ ëˆˆì„ ê°€ì§„ ê·€ì—¬ìš´ ê³ ì–‘ì´')"
        )
        top_k = st.slider("ê²€ìƒ‰í•  ìƒìœ„ ì´ë¯¸ì§€ ê°œìˆ˜", 1, 10, 3)

        if st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰"):
            translated_query = translator.translate(query)
            st.write(f"{query} {translated_query}")
            results = app.search(translated_query, top_k)
            if results:
                st.write("### ê²€ìƒ‰ ê²°ê³¼:")
                cols = st.columns(top_k)

                for idx, (img, name, score) in enumerate(results):
                    cols[idx].image(
                        img,
                        caption=f"{name}\nìœ ì‚¬ë„: {score:.3f}",
                        use_container_width=True,
                    )


if __name__ == "__main__":
    main()
